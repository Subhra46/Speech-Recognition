# -*- coding: utf-8 -*-
"""MINOR ML3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eic6SSFjZa3S5NkZy5z2BI-AWdEpT0cW

IMPORT THE RELEVANT LIBRARIES
"""

import tensorflow as tf
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import nltk
import re
import string
string.punctuation
nltk.download('stopwords')
from google.colab import files
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction import text
stop = text.ENGLISH_STOP_WORDS
from nltk.corpus import stopwords
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import Dense,Input,LSTM
from tensorflow.keras.models import  Model
import io

"""LOAD THE DATA"""

uploaded = files.upload() #for uploading data into google colab from local machine

df = pd.read_csv(io.BytesIO(uploaded['labeled_data.csv']),encoding='ISO-8859-1') #loading the data into dataframe df

df1=df.copy() #copying the contents of df into df1,creating a checkpoint

"""DATA PREPROCESSING"""

def remove_mentionUrls(text): #removing url
  tweet_out = re.sub(r'@[A-Za-z0-9]+','',text)
  re.sub('https?://[A-Za-z0-9./]+','',tweet_out)
  return tweet_out
def remove_nonalphanumeric(text):  #removing non alphanumeric characters
  text_out ="".join([char for char in text if char not in string.punctuation])
  return text_out
df1['Tweets_nourl']=df1['tweet'].apply(lambda x: remove_mentionUrls(x))
df1['Tweets_punc']=df1['Tweets_nourl'].apply(lambda x: remove_nonalphanumeric(x))  
 
df1.head(20)

stop = stopwords.words('english')    #stopwords removal
test = pd.DataFrame(df1)
test.columns = ['Unnamed: 0', 'count', 'hate_speech', 'offensive_language', 'neither',
       'class', 'tweet','Tweets_nourl',
       'Tweets_punc']

# Exclude stopwords with Python's list comprehension and pandas.DataFrame.apply.
test['tweet_without_stopwords'] = test['Tweets_punc'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))
print(test)
df1.head(20)

df1['tweet_type'] = df1['class'].map({2:"neither",1:"offensive",0:"hate"}) #creating a new column tweet_type by mapping {2:"neither",1:"offensive",0:"hate"}

df1.head()

df1['tweet_type'].unique

data=df1[['tweet_without_stopwords','tweet_type']] #creating a new dataframe named data for our learning model by extracting neccessary columns from df1

data.head(15)

"""TRAIN AND TEST DATA"""

X=data['tweet_without_stopwords']   # selecting inputs 
Y=data['tweet_type']                # selecting outputs
X = [str (item) for item in X]

from sklearn import preprocessing


label_encoder = preprocessing.LabelEncoder() #creating instance of Label Encoder


Y= label_encoder.fit_transform(Y)           #encoding the output classes



X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20, random_state=42) #splitting the data into train and test sets

from keras.utils import to_categorical
Y_train = to_categorical(Y_train)
Y_test = to_categorical(Y_test)

"""TOKENIZER"""

tokenizer = Tokenizer(num_words=50000)   #let us take 50,000 as number of words to be safe as we have previously checked that our data contains around 20,000 words
tokenizer.fit_on_texts(X_train)


X_train = tokenizer.texts_to_sequences(X_train)
X_test = tokenizer.texts_to_sequences(X_test)

vocab_size = len(tokenizer.word_index) + 1

maxlen=200
X_train = pad_sequences(X_train, padding='post',maxlen=maxlen,truncating='post')  #padding the sequences 
X_test = pad_sequences(X_test, padding='post',maxlen=maxlen,truncating='post')    #padding the sequences
#this will pad the sequences of sentences having less than 200 words with zeros at the end and if any sentences has more than 200 words,it will be truncated at the end

"""GloVe"""

!wget http://nlp.stanford.edu/data/glove.6B.zip

import zipfile
zip_ref = zipfile.ZipFile("glove.6B.zip", 'r')
zip_ref.extractall('/content')
zip_ref.close()

from numpy import array
from numpy import asarray
from numpy import zeros
from numpy import array
from keras.preprocessing.text import one_hot
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers.core import Activation, Dropout, Dense
from keras.layers import Flatten, LSTM
from keras.layers import GlobalMaxPooling1D
from keras.models import Model
from keras.layers.embeddings import Embedding
from sklearn.model_selection import train_test_split
from keras.preprocessing.text import Tokenizer
from keras.layers import Input
from keras.layers.merge import Concatenate

import pandas as pd
import numpy as np
import re

embeddings_dictionary = dict()
with io.open('glove.6B.300d.txt', encoding='utf8') as glove_file:



  for line in glove_file:

    records = line.split()
    word = records[0]
    vector_dimensions = asarray(records[1:], dtype='float32')
    embeddings_dictionary [word] = vector_dimensions

glove_file.close()

embedding_matrix = zeros((vocab_size, 300))
for word, index in tokenizer.word_index.items():
    embedding_vector = embeddings_dictionary.get(word)
    if embedding_vector is not None:
        embedding_matrix[index] = embedding_vector

"""MODEL USING RNN(LSTM)"""

deep_inputs = Input(shape=(maxlen,))
embedding_layer = Embedding(vocab_size, 300, weights=[embedding_matrix], trainable=False)(deep_inputs) #Embedding layer
LSTM_Layer_1 = LSTM(128)(embedding_layer) #Lstm layer with width of 128 neurons
dense_layer_1 = Dense(3, activation='softmax')(LSTM_Layer_1) #Dense layer for out with softmax activation
model = Model(inputs=deep_inputs, outputs=dense_layer_1)

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

NUM_OF_EPOCHS=10 #hyperparameter
BATCH_SIZE=128    #hyperparameter
r=model.fit(X_train,Y_train,validation_data=(X_test,Y_test),epochs=NUM_OF_EPOCHS,batch_size=BATCH_SIZE)

PLOTTING THE RESULTS

import matplotlib.pyplot as plt
history=r
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])

plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train','test'], loc='upper left')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])

plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train','test'], loc='upper left')
plt.show()